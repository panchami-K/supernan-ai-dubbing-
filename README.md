# ğŸ¬ Supernan Golden 15 Seconds - AI Dubbing Pipeline

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> **Production-grade multilingual AI video dubbing with intelligent segment selection**

## ğŸŒŸ Features

- ğŸ¯ **Auto Language Detection**: Supports 99+ languages via Whisper
- ğŸ§  **AI-Powered Segment Selection**: Automatically finds the best 15-30 seconds
- ğŸ—£ï¸ **Voice Cloning**: Preserves original speaker's tone and style
- ğŸ‘„ **Perfect Lip Sync**: Advanced Wav2Lip integration
- âœ¨ **Face Enhancement**: GFPGAN for professional video quality
- ğŸŒ **Any-to-Any Translation**: Kannadaâ†’Hindi, Englishâ†’Hindi, etc.
- ğŸ–¥ï¸ **User-Friendly UI**: Streamlit web interface

## ğŸ“ Repository Structure
supernan-ai-dubbing-/
â”œâ”€â”€ src/                    # Core pipeline modules
â”‚   â”œâ”€â”€ config.py          # Configuration management
â”‚   â”œâ”€â”€ transcribe.py      # Speech-to-text (Whisper)
â”‚   â”œâ”€â”€ segment_analyzer.py # Best segment selection
â”‚   â”œâ”€â”€ translate.py       # Neural machine translation
â”‚   â”œâ”€â”€ voice_clone.py     # XTTS voice cloning
â”‚   â”œâ”€â”€ lip_sync.py        # Wav2Lip integration
â”‚   â”œâ”€â”€ face_restore.py    # GFPGAN face enhancement
â”‚   â”œâ”€â”€ video_utils.py     # Video processing utilities
â”‚   â””â”€â”€ pipeline.py        # Main orchestrator
â”œâ”€â”€ web_app/               # Streamlit UI
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ data/                  # Input/output videos
â”œâ”€â”€ dub_video.py          # CLI entry point
â””â”€â”€ requirements.txt

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/panchami-K/supernan-ai-dubbing-.git
cd supernan-ai-dubbing-

# Install dependencies
pip install -r requirements.txt

# Download models (one-time)
python scripts/download_models.py

CLI Usage
# Full pipeline on video
python dub_video.py --input video.mp4 --target-lang hi --output dubbed.mp4

# With custom segment selection
python dub_video.py --input video.mp4 --start 45 --end 60 --target-lang hi

Web UI
bash
streamlit run web_app/app.py

ğŸ—ï¸ Pipeline Architecture
plain
Copy
Input Video (Any Language)
    â†“
[Transcribe] â†’ Whisper (Auto-detect language)
    â†“
[Analyze] â†’ AI selects best 15-30s segment
    â†“
[Translate] â†’ IndicTrans2 / NLLB
    â†“
[Voice Clone] â†’ XTTS (preserves speaker tone)
    â†“
[Lip Sync] â†’ Wav2Lip (matches lips to audio)
    â†“
[Enhance] â†’ GFPGAN (face restoration)
    â†“
Output Video (Target Language)


ğŸ‘¤ Author
Panchami K
Email: panchamik12345@gmail.com
GitHub: @panchami-K
ğŸ“œ License
MIT License - see LICENSE file


## âœ… True visual lip-sync (important)
If your output is only **video + dubbed audio** but lips do not move, the model did not run.
Use the built-in CLI below, which calls VideoReTalking directly and fails loudly (no fake fallback):

```bash
python dub_video.py   --face data/temp/segment_37_52.mp4   --audio output/hindi_dubbed.wav   --output output/final_hindi_lipsync.mp4
```

### Why your previous notebook produced no lip-sync
- It generated a custom `inference_simple.py` that draws synthetic mouth ellipses instead of running the real model.
- It used placeholder checkpoint IDs for several models.
- Dependency install errors were ignored, so execution continued in a broken environment.
- On failure it silently fell back to ffmpeg audio replacement.

The updated pipeline removes silent fallback for this step so failures are visible and actionable.
